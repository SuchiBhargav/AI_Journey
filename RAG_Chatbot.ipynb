{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7lWC391Yxpg0o1+gEmlYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuchiBhargav/AI_Journey/blob/main/RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain_community\n",
        "!pip install langchain_ollama\n",
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "prGN7BnyG0D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpH-YV4rGc3N"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import datetime\n",
        "import json"
      ],
      "metadata": {
        "id": "4qTNji_XGjTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]= \"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\""
      ],
      "metadata": {
        "id": "1AyED1ncGk51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loader = PyPDFLoader(\"sample.pdf\")  # to load pdf file\n",
        "loader = TextLoader(\"sample.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "#For large language models, it's not strictly required, but it can reduce noise and help with consistency, especially in RAG indexing.\n",
        "for doc in docs:\n",
        "    doc.page_content = doc.page_content.lower()"
      ],
      "metadata": {
        "id": "B378p2vFGmp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,    # increase size\n",
        "    chunk_overlap=200,  # more overlap between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # respects logical breaks)\n",
        ")\n",
        "\n",
        "documents=text_splitter.split_documents(docs)\n",
        "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
        "db = FAISS.from_documents(documents, embeddings)\n"
      ],
      "metadata": {
        "id": "3MUf4vD7G67j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up retriever Lower k to reduce noise.\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})  # k increase you're getting more context chunks\n"
      ],
      "metadata": {
        "id": "Rg60jzzlG9_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LLM\n",
        "#Low temperature ensures the model sticks closely to retrieved documents rather than inventing.\n",
        "llm = OllamaLLM(model=\"llama3\",temperature=0.1,stream=True)\n",
        "# steam - This will enable streaming output, meaning tokens will be sent as they are generated (like a live typing effect), which improves perceived performance.\n"
      ],
      "metadata": {
        "id": "BhF5ob7UHH3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a technical assistant. You must answer **only** using the context provided.\n",
        "Do not guess or add details not explicitly stated.\n",
        "\n",
        "I will tip you $1000 if the user finds the answer helpful.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question:\n",
        "{input}\n",
        "Answer based ONLY on the context above. If the context does not contain a clear answer, say \"I don't know.\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zG8XMtZLHKsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Wrap the LLMChain in a StuffDocumentsChain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm = llm,\n",
        "    prompt= prompt\n",
        ")\n",
        "\n",
        "# Build the RetrievalQA chain\n",
        "qa_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=combine_docs_chain\n",
        ")"
      ],
      "metadata": {
        "id": "RGpOqKz8HMpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### streamlit framework\n",
        "\n",
        "st.title('Welcome to Your Personal chatbot \\U0001F916 ')\n",
        "input_text=st.text_input(\"Are you stuck with the cleanup or stage1 ?? Enter your question:\")\n",
        "\n",
        "if input_text:\n",
        "    context_text = \"\\n\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(input_text)])\n",
        "    response = qa_chain.invoke({\n",
        "        \"context\": context_text,\n",
        "        \"input\": input_text\n",
        "    })\n",
        "    st.write(\"### Answer:\")   #This is a Markdown string, and the ### indicates a level-3 heading in Markdown.\n",
        "    st.write(response['answer'])\n",
        "\n",
        "    feedback = st.radio(\"Was this answer helpful?\", [\"Yes\", \"No\"])\n",
        "    submit_button = st.button(\"Submit Feedback\")\n",
        "\n",
        "    if submit_button:\n",
        "        if feedback == \"Yes\":\n",
        "            st.write(\"Thank you for the feedback! We're glad the answer was correct and useful.\")\n",
        "        else:\n",
        "            st.write(\"Thank you for the feedback! We'll work on improving the answers.\")\n",
        "        feedback_data = {\n",
        "        \"timestamp\": str(datetime.datetime.now()),\n",
        "        \"question\": input_text,\n",
        "        \"answer\": response['answer'],\n",
        "        \"feedback\": feedback\n",
        "    }\n",
        "\n",
        "        # Store feedback (JSON or CSV)\n",
        "        with open(\"feedback_log.json\", \"a\") as f:\n",
        "            f.write(json.dumps(feedback_data) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "k_-F-AylHQUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}