{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeQnSwR0L36+5Cbjdixun6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuchiBhargav/AI_Journey/blob/main/RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain_community\n",
        "!pip install langchain_ollama\n",
        "!pip install streamlit\n",
        "!pip install redis\n",
        "!brew install redis\n",
        "# brew services start redis\n",
        "# brew services list\n",
        "#steamlit run RAG_Chatbot.py\n",
        "#ollama run llama3"
      ],
      "metadata": {
        "id": "prGN7BnyG0D4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpH-YV4rGc3N"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "import redis\n",
        "import hashlib"
      ],
      "metadata": {
        "id": "4qTNji_XGjTM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]= \"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\""
      ],
      "metadata": {
        "id": "1AyED1ncGk51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cache implemented for fast retrievl\n",
        "cache = redis.Redis(host='localhost', port=6379, db=0)\n",
        "\n",
        "def get_cache_key(question: str) -> str:\n",
        "    \"\"\"Create a consistent key using a hash.\"\"\"\n",
        "    return \"q:\" + hashlib.sha256(question.lower().strip().encode()).hexdigest()\n",
        "\n",
        "def get_cached_answer(question: str):\n",
        "    key = get_cache_key(question)\n",
        "    value = cache.get(key)\n",
        "    if value:\n",
        "        return value.decode()\n",
        "    return None\n",
        "\n",
        "def set_cached_answer(question: str, answer: str):\n",
        "    key = get_cache_key(question)\n",
        "    cache.setex(key, 86400, answer)  # Cache for 1 day (optional TTL)"
      ],
      "metadata": {
        "id": "7PPbbhl3k349"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OllamaEmbeddings(model=\"llama3\")"
      ],
      "metadata": {
        "id": "3MUf4vD7G67j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VECTORSTORE_PATH = \"faiss_index\"\n",
        "if os.path.exists(VECTORSTORE_PATH):\n",
        "    db = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "else:\n",
        "        loader = PyPDFLoader(\"mgen.pdf\")\n",
        "        # loader = TextLoader(\"mgen.txt\")\n",
        "        docs = loader.load()\n",
        "\n",
        "        #For large language models, it's not strictly required, but it can reduce noise and help with consistency, especially in RAG indexing.\n",
        "        for doc in docs:\n",
        "            doc.page_content = doc.page_content.lower()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1500,    # increase size\n",
        "            chunk_overlap=200,  # more overlap between chunks\n",
        "            separators = [\"\\n\\n\", \"\\n\", \".\", \";\"] ,  # respects logical breaks)\n",
        "\n",
        "        )\n",
        "\n",
        "        documents=text_splitter.split_documents(docs)\n",
        "        # This embeds the documents and builds the vector store\n",
        "        db = FAISS.from_documents(documents, embeddings)\n",
        "        db.save_local(VECTORSTORE_PATH)\n"
      ],
      "metadata": {
        "id": "3Ck-iCzLlPFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set up retriever Lower k to reduce noise.\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})  # k increase you're getting more context chunks\n",
        "\n",
        "# Define the LLM\n",
        "#Low temperature ensures the model sticks closely to retrieved documents rather than inventing.\n",
        "llm = OllamaLLM(model=\"llama3\",temperature=0.1,stream=True)\n",
        "# steam - This will enable streaming output, meaning tokens will be sent as they are generated (like a live typing effect), which improves perceived performance.\n"
      ],
      "metadata": {
        "id": "jbmrpeOGlQFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a technical assistant. You must answer **only** using the context provided.\n",
        "Do not guess or add details not explicitly stated.\n",
        "If the context does not include the answer, respond only with: \"I don't know.\"\n",
        "I will tip you $1000 if the user finds the answer helpful.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "Question:\n",
        "{input}\n",
        "Answer based ONLY on the context above. If the context does not contain a clear answer, say \"I don't know.\"\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "pwms2XHWlS2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the LLMChain in a StuffDocumentsChain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm = llm,\n",
        "    prompt= prompt\n",
        ")\n",
        "\n",
        "# Build the RetrievalQA chain\n",
        "qa_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=combine_docs_chain\n",
        ")"
      ],
      "metadata": {
        "id": "XPgkF9Q2lYt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\"\n",
        "\n",
        "def submit_feedback():\n",
        "    feedback_data = {\n",
        "        \"question\": st.session_state.input_text,\n",
        "        \"answer\": st.session_state.last_answer,\n",
        "        \"feedback\": st.session_state.feedback,\n",
        "        \"context\" : context_text\n",
        "    }"
      ],
      "metadata": {
        "id": "LSXrej6KlbdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    if st.session_state.feedback == \"Yes\":\n",
        "        st.write(\"Thank you for the feedback! We're glad the answer was correct and useful.\")\n",
        "    else:\n",
        "        st.write(\"Thank you for the feedback! We'll work on improving the answers.\")\n",
        "\n",
        "    if os.path.exists(\"feedback_log.json\"):\n",
        "        with open(\"feedback_log.json\", \"r\") as f:\n",
        "            try:\n",
        "                feedback_list = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                feedback_list = []\n",
        "    else:\n",
        "        feedback_list = []\n",
        "\n",
        "    feedback_list.append(feedback_data)\n",
        "\n",
        "    with open(\"feedback_log.json\", \"w\") as f:\n",
        "        json.dump(feedback_list, f, indent=2)\n",
        "\n",
        "    st.session_state.input_text = \"\"  # âœ… safely reset input before re-render\n"
      ],
      "metadata": {
        "id": "DAaYZthLlcWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# streamlit framework for ui design\n",
        "st.title('Welcome to the MgenAi chatbot \\U0001F916 ')\n",
        "\n",
        "# Initialize session state\n",
        "if \"input_text\" not in st.session_state:\n",
        "    st.session_state.input_text = \"\"\n",
        "if \"last_answer\" not in st.session_state:\n",
        "    st.session_state.last_answer = \"\"\n",
        "if \"feedback\" not in st.session_state:\n",
        "    st.session_state.feedback = \"Yes\""
      ],
      "metadata": {
        "id": "DMEn3CXNlrAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text input\n",
        "input_text = st.text_input(\n",
        "    \"Are you stuck with the cleanup or stage1 ?? Enter your question:\",\n",
        "    key=\"input_text\"\n",
        ")\n",
        "\n",
        "if input_text:\n",
        "    cache_response = get_cached_answer(input_text)  #checking in cache\n",
        "    if cache_response:\n",
        "        st.session_state.last_answer = cache_response\n",
        "        st.write(\"### Answer:\")\n",
        "        st.write(st.session_state.last_answer)\n",
        "    else:\n",
        "        context_text = \"\\n\\n\".join([doc.page_content for doc in retriever.get_relevant_documents(input_text)])\n",
        "        response = qa_chain.invoke({\n",
        "            \"context\": context_text,\n",
        "            \"input\": input_text\n",
        "        })\n",
        "        st.session_state.last_answer = response['answer']\n",
        "        set_cached_answer(input_text,st.session_state.last_answer)  #saving in cahe\n",
        "        st.write(\"### Answer:\")\n",
        "        st.write(st.session_state.last_answer)\n",
        "\n",
        "    st.radio(\"Was this answer helpful?\", [\"Yes\", \"No\"], key=\"feedback\")\n",
        "    st.button(\"Submit Feedback\", on_click=submit_feedback)"
      ],
      "metadata": {
        "id": "zieR2Ihnlulz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}